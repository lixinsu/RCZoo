#!/usr/bin/env python
# coding: utf-8

import json
from .tokenization import (BasicTokenizer,
                             BertTokenizer,
                             whitespace_tokenize)

class SquadExample(object):
    """
    A single training/test example for the Squad dataset.
    For examples without an answer, the start and end position are -1.
    """

    def __init__(self,
                 qas_id,
                 question_text,
                 doc_tokens,
                 orig_answer_text=None,
                 start_position=None,
                 end_position=None,
                 is_impossible=None):
        self.qas_id = qas_id
        self.question_text = question_text
        self.doc_tokens = doc_tokens
        self.orig_answer_text = orig_answer_text
        self.start_position = start_position
        self.end_position = end_position
        self.is_impossible = is_impossible

    def __str__(self):
        return self.__repr__()

    def __repr__(self):
        s = ""
        s += "qas_id: %s\n" % (self.qas_id)
        s += ", question_text: %s\n" % (
            self.question_text)
        s += ", doc_tokens: [{}]\n".format(" ".join(self.doc_tokens))
        if self.start_position:
            s += ", start_position: {}\n" .format (self.start_position)
        if self.end_position:
            s += ", end_position: {}\n".format (self.end_position)
        if self.is_impossible:
            s += ", is_impossible: {}\n".format  (self.is_impossible)
        return s


def is_whitespace(c):
    if c == " " or c == "\t" or c == "\r" or c == "\n" or ord(c) == 0x202F:
        return True
    return False

def split_by_space(paragraph_text):
    doc_tokens = []
    char_to_word_offset = []
    prev_is_whitespace = True
    for c in paragraph_text:
        if is_whitespace(c):
            prev_is_whitespace = True
        else:
            if prev_is_whitespace:
                doc_tokens.append(c)
            else:
                doc_tokens[-1] += c
            prev_is_whitespace = False
        char_to_word_offset.append(len(doc_tokens) - 1)
    return doc_tokens, char_to_word_offset

def read_squad_examples(input_file, is_training, version_2_with_negative, debug=False):
    """Read a SQuAD json file into a list of SquadExample."""
    with open(input_file, "r", encoding='utf-8') as reader:
        input_data = json.load(reader)["data"]

    examples = []
    for entry in input_data:
        for paragraph in entry["paragraphs"]:
            paragraph_text = paragraph["context"]
            doc_tokens, char_to_word_offset = split_by_space(paragraph_text)
            for qa in paragraph["qas"]:
                qas_id = qa["id"]
                question_text = qa["question"]
                start_position = None
                end_position = None
                orig_answer_text = None
                is_impossible = False
                if is_training:
                    if version_2_with_negative:
                        is_impossible = qa["is_impossible"]
                    if (len(qa["answers"]) != 1) and (not is_impossible):
                        raise ValueError(
                            "For training, each question should have exactly 1 answer.")
                    if not is_impossible:
                        answer = qa["answers"][0]
                        orig_answer_text = answer["text"]
                        answer_offset = answer["answer_start"]
                        answer_length = len(orig_answer_text)
                        start_position = char_to_word_offset[answer_offset]
                        end_position = char_to_word_offset[answer_offset + answer_length - 1]
                        # Only add answers where the text can be exactly recovered from the
                        # document. If this CAN'T happen it's likely due to weird Unicode
                        # stuff so we will just skip the example.
                        #
                        # Note that this means for training mode, every example is NOT
                        # guaranteed to be preserved.
                        actual_text = " ".join(doc_tokens[start_position:(end_position + 1)])
                        cleaned_answer_text = " ".join(
                            whitespace_tokenize(orig_answer_text))
                        if actual_text.find(cleaned_answer_text) == -1:
                            logger.warning("Could not find answer: '%s' vs. '%s'",
                                           actual_text, cleaned_answer_text)
                            continue
                    else:
                        start_position = -1
                        end_position = -1
                        orig_answer_text = ""

                example = SquadExample(
                    qas_id=qas_id,
                    question_text=question_text,
                    doc_tokens=doc_tokens,
                    orig_answer_text=orig_answer_text,
                    start_position=start_position,
                    end_position=end_position,
                    is_impossible=is_impossible)
                examples.append(example)
    return examples


def read_multi_examples(input_file, is_training, version_2_with_negative=False, debug=False):
    input_data = [json.loads(line) for line in open(input_file)]
    examples = []
    for entry in input_data:
        paragraph_text = entry['passage']
        doc_tokens, char_to_word_offset = split_by_space(paragraph_text)
        qas_id = entry['query_id']
        question_text = entry['query']
        is_impossible = False
        if is_training:
            start_position = [char_to_word_offset[x[0]]for x in entry['positions']]
            end_position = [char_to_word_offset[x[1]-1]for x in entry['positions']]
            orig_answer_text = entry['answers']
        else:
            start_position = None
            end_position = None
            orig_answer_text = None
        example = SquadExample(
                qas_id=qas_id,
                question_text=question_text,
                doc_tokens=doc_tokens,
                orig_answer_text=orig_answer_text,
                start_position=start_position,
                end_position=end_position,
                is_impossible=is_impossible)
        examples.append(example)
    return examples


def read_marco_examples(input_file, is_training, version_2_with_negative=None, debug=False):
  """Read a MARCO json file into a list of SquadExample."""
  def is_whitespace(c):
    if c == " " or c == "\t" or c == "\r" or c == "\n" or ord(c) == 0x202F:
      return True
    return False

  examples = []
  cnt = 128
  for entry in open(input_file):
    cnt -= 1
    if debug and cnt ==0:
        break
    data = json.loads(entry)
    paragraph_text = data["passage"]
    doc_tokens = []
    char_to_word_offset = []
    prev_is_whitespace = True
    for c in paragraph_text:
      if is_whitespace(c):
        prev_is_whitespace = True
      else:
        if prev_is_whitespace:
          doc_tokens.append(c)
        else:
          doc_tokens[-1] += c
        prev_is_whitespace = False
      char_to_word_offset.append(len(doc_tokens) - 1)

    qas_id = data["id"]
    question_text = data["query"]
    start_position = None
    end_position = None
    orig_answer_text = None
    is_impossible = False
    if is_training:
      is_impossible = False
      if not is_impossible:
        orig_answer_text = data["answer_text"]
        answer_offset = data["answer_start"]
        answer_length = len(orig_answer_text)
        start_position = char_to_word_offset[answer_offset]
        end_position = char_to_word_offset[answer_offset + answer_length - 1]
        # Only add answers where the text can be exactly recovered from the
        # document. If this CAN'T happen it's likely due to weird Unicode
        # stuff so we will just skip the example.
        #
        # Note that this means for training mode, every example is NOT
        # guaranteed to be preserved.
        actual_text = " ".join(
            doc_tokens[start_position:(end_position + 1)])
        cleaned_answer_text = " ".join(
            whitespace_tokenize(orig_answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
          print("Could not find answer: '%s' vs. '%s'",
                             actual_text, cleaned_answer_text)
          continue
      else:
        start_position = -1
        end_position = -1
        orig_answer_text = ""

    example = SquadExample(
        qas_id=qas_id,
        question_text=question_text,
        doc_tokens=doc_tokens,
        orig_answer_text=orig_answer_text,
        start_position=start_position,
        end_position=end_position,
        is_impossible=is_impossible)
    examples.append(example)
  print('\nHas {} examples\n'.format(len(examples)))
  return examples
